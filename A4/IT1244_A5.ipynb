{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yRj6Htiebfn"
      },
      "source": [
        "\n",
        "# Programming Assignment 5 - Linear Regression\n",
        "\n",
        "---\n",
        "\n",
        "## Boston Housing Dataset\n",
        "\n",
        "Each instance in this dataset describes the attributes of a housing property in Boston suburb and the task is to predict the house prices in thousands of dollars. There are 13 numerical attributes with varying scales describing the housing properties of suburbs. The readme file in the dataset provides details on the dataset. Read this file before implementing your algorithm to check whether you need to do any data preprocessing steps. The 13 attributes of each property and the housing price (in $1000's) are given as follows:\n",
        "1. CRIM - per capita crime rate by town\n",
        "2. ZN - proportion of residential land zoned for lots over 25,000 sq.ft.\n",
        "3. INDUS - proportion of non-retail business acres per town\n",
        "4. CHAS - Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
        "5. NOX - nitric oxides concentration (parts per 10 million)\n",
        "6. RM - average number of rooms per dwelling\n",
        "7. AGE - proportion of owner-occupied units built prior to 1940\n",
        "8. DIS - weighted distances to five Boston employment centres\n",
        "9. RAD - index of accessibility to radial highways\n",
        "10. TAX - full-value property-tax rate per $10,000\n",
        "11. PTRATIO - pupil-teacher ratio by town\n",
        "12. B - 1000 (Bk - 0.63) ^ 2 where Bk is the proportion of blacks by town\n",
        "13. LSTAT - % lower status of the population\n",
        "14. MEDV - Median value of owner-occupied homes in $1000's\n",
        "\n",
        "`housing.data` contains the actual dataset.\n",
        "\n",
        "**IMPORTANT**: you can assume that the last column will always contain the y values. However, your code will be tested against datasets of varying number of rows and columns, so do not hardcode (e.g. `[:13]`)\n",
        "\n",
        "## Objective\n",
        "\n",
        "You are to implement a linear regression algorithm using gradient descent to predict the price of a house given the details of the property. After completing this assignment, you should be familiar with the following:\n",
        "1. Loading a dataset\n",
        "2. Normalising a dataset\n",
        "3. Splitting a dataset into training and testing set\n",
        "4. Computing the RMSE between a predicted value and actual value\n",
        "5. Implementing gradient descent to find optimal parameters\n",
        "6. Applying linear regression with optimal parameters for prediction\n",
        "\n",
        "### **Total Marks: 30**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JA3DODTROxzG"
      },
      "source": [
        "\n",
        "## Downloading the Dataset and Importing Modules\n",
        "\n",
        "You can follow the steps below to download the dataset and upload it to a Colab environment.\n",
        "1. Download the dataset from https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data (save it as a text file)\n",
        "2. Open the Colab file browser by pressing the small folder icon on the top left of the Colab page.  \n",
        "3. Drag and drop the `housing.data` file into the Colab folder `/content/`.\n",
        "\n",
        "We will be using `math` and `numpy` as `np` for the questions. **You do not need to import them when submitting on Coursemology.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 235,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5c4R1cOPhFMM",
        "outputId": "7f166e8a-db2c-4040-fe1e-6435310c2d82"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "\n",
        "# to display the float numbers with 2 decimal points and supress the use of\n",
        "# scientific notations for small numbers\n",
        "## np.set_printoptions(precision=2, suppress=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DANbMtO1Bqx"
      },
      "source": [
        "---\n",
        "\n",
        "### Q1 load_data (3 marks)\n",
        "\n",
        "We first need to load the dataset from `housing.data` and store the relevant data in numpy arrays.\n",
        "\n",
        "The function `load_data` takes in a **text** file `filename` and returns the numpy array `dataset`, containing the contents of the dataset as type **float**. Please **leave the rows and columns in the order that they appear** in the text file.\n",
        "\n",
        "Tip: there's a numpy function that reads text files and converts their contents into arrays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 236,
      "metadata": {
        "id": "Og4vGG41a3tr"
      },
      "outputs": [],
      "source": [
        "# Submit to Coursemology\n",
        "def load_data(filename):\n",
        "    '''\n",
        "    filename: string, the path to the house.data file\n",
        "    RETURN\n",
        "        dataset: numpy array, shape = [N, 14]\n",
        "    '''\n",
        "    with open(filename[1:], 'rb') as f:\n",
        "        dataset = np.loadtxt(f)\n",
        "        # for line in f.readline():\n",
        "        #     dataset.append(np.fromstring(line, sep=' '))\n",
        "    ## start your code here\n",
        "    \n",
        "    \n",
        "    ## end\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 237,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3l0xTXhg4zfg",
        "outputId": "0f9a0c76-ef2b-4ae3-dbcb-c848bad8d62d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(506, 14)\n",
            "[3.61352356e+00 1.13636364e+01 1.11367787e+01 6.91699605e-02\n",
            " 5.54695059e-01 6.28463439e+00 6.85749012e+01 3.79504269e+00\n",
            " 9.54940711e+00 4.08237154e+02 1.84555336e+01 3.56674032e+02\n",
            " 1.26530632e+01 2.25328063e+01]\n"
          ]
        }
      ],
      "source": [
        "# Testing\n",
        "\n",
        "filename = \"/content/housing.data\" \n",
        "dataset = load_data(filename)\n",
        "\n",
        "print(dataset.shape)\n",
        "print(np.mean(dataset, axis=0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkOB2VCV01nz"
      },
      "source": [
        "Expected output:\n",
        "```\n",
        "(506, 14)\n",
        "[3.61352356e+00 1.13636364e+01 1.11367787e+01 6.91699605e-02\n",
        " 5.54695059e-01 6.28463439e+00 6.85749012e+01 3.79504269e+00\n",
        " 9.54940711e+00 4.08237154e+02 1.84555336e+01 3.56674032e+02\n",
        " 1.26530632e+01 2.25328063e+01]\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6so9ooq3092o"
      },
      "source": [
        "---\n",
        "\n",
        "### Q2 normalize (3 marks)\n",
        "\n",
        "As per usual, we will now feature scale our data, but this time using normalisation. Each column in `dataset` needs to be normalized separately. That is, for each column, for each value, we subtract the minimum from that column and then divide by (max of column - min of column).\n",
        "\n",
        "The function `normalize` takes in a `dataset` and returns the normalized dataset `dataset_normalized`. **The last column (i.e. the y values) should not be normalized**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 238,
      "metadata": {
        "id": "PxoCoHXP42T1"
      },
      "outputs": [],
      "source": [
        "# Submit to Coursemology\n",
        "def normalize(dataset):\n",
        "    '''\n",
        "    dataset: numpy array, shape = [N, D]\n",
        "    RETURN\n",
        "        dataset_normalized: numpy array, shape = [N, D]\n",
        "    '''\n",
        "    dataset_normalized = np.zeros_like(dataset)\n",
        "    for col_idx in range(dataset.shape[1]-1):\n",
        "        dataset_normalized[:, col_idx] = (dataset[:, col_idx] - np.min(dataset[:, col_idx])) / (np.max(dataset[:, col_idx]) - np.min(dataset[:, col_idx]))\n",
        "\n",
        "    dataset_normalized[:, -1] = dataset[:, -1]\n",
        "    ## start your code here\n",
        "    \n",
        "    \n",
        "    ## end\n",
        "    return dataset_normalized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 239,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXoRu6qrkb9B",
        "outputId": "c1651596-b25c-4004-a3b3-f2fee447a1a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(506, 14)\n",
            "[ 0.0405441   0.11363636  0.39137752  0.06916996  0.34916679  0.52186901\n",
            "  0.67636355  0.24238128  0.37171335  0.42220831  0.62292911  0.89856783\n",
            "  0.30140903 22.53280632]\n"
          ]
        }
      ],
      "source": [
        "# Testing\n",
        "\n",
        "dataset_normalized = normalize(dataset)\n",
        "print(dataset_normalized.shape)\n",
        "print(np.mean(dataset_normalized,axis=0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvMVe-x3MkgU"
      },
      "source": [
        "Expected output:\n",
        "```\n",
        "(506, 14)\n",
        "[ 0.0405441   0.11363636  0.39137752  0.06916996  0.34916679  0.52186901\n",
        "  0.67636355  0.24238128  0.37171335  0.42220831  0.62292911  0.89856783\n",
        "  0.30140903 22.53280632]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_RSjrhbRHTt"
      },
      "source": [
        "---\n",
        "\n",
        "### Q3 add_bias (3 marks)\n",
        "\n",
        "Now, we need to add a bias to each row for the linear regression to include the weight w0.\n",
        "\n",
        "The function `add_bias` takes in a `dataset` and returns the dataset with an additional first column of bias 1 `dataset_ones` (i.e. with a R x C dataset, it should return a R x (C + 1) dataset with the leftmost column being all 1s)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 240,
      "metadata": {
        "id": "2cEDfBaM46PT"
      },
      "outputs": [],
      "source": [
        "# Submit to Coursemology\n",
        "def add_bias(dataset):\n",
        "    '''\n",
        "    dataset: numpy array, shape = [N,D]\n",
        "    RETURN\n",
        "        dataset_ones: numpy array, shape = [N, D+1]\n",
        "    '''\n",
        "    dataset_ones = np.ones((dataset.shape[0], dataset.shape[1]+1))\n",
        "    dataset_ones[:, 1:] = dataset[:, :]\n",
        "    ## Start your code here\n",
        "    \n",
        "\n",
        "    ## end\n",
        "    return dataset_ones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 241,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48zz5lDbow4x",
        "outputId": "e130f12e-759e-4cea-ae62-cf225500f85c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(506, 15)\n",
            "[ 1.          0.01399878  0.          0.28152493  0.          0.31481481\n",
            "  0.38493964  0.98043254  0.24264111  0.13043478  0.22900763  0.89361702\n",
            "  0.9487367   0.53228477 13.6       ]\n"
          ]
        }
      ],
      "source": [
        "# Testing\n",
        "\n",
        "dataset_ones = add_bias(dataset_normalized)\n",
        "print(dataset_ones.shape)\n",
        "print(dataset_ones[20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hg0PLrsUSFt3"
      },
      "source": [
        "Expected output:\n",
        "```\n",
        "(506, 15)\n",
        "[ 1.          0.01399878  0.          0.28152493  0.          0.31481481\n",
        "  0.38493964  0.98043254  0.24264111  0.13043478  0.22900763  0.89361702\n",
        "  0.9487367   0.53228477 13.6       ]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EEdmQoxyQcF"
      },
      "source": [
        "---\n",
        "\n",
        "### Q4 split_dataset (3 marks)\n",
        "\n",
        "We have one last data manipulation to complete before training our model - splitting the dataset into a training set and a testing set.\n",
        "\n",
        "The function `split_dataset` takes in a `dataset` and returns the training set comprising of arrays `train_X` and `train_y`, and the testing set comprising of arrays `test_X` and `test_y`. The training set is made up of the **first 80%** of the dataset whiled the testing set is made up of the remaining 20%. X refers to all the columns except the last one, and y refers to the last column.\n",
        "\n",
        "Tip: `np.shape()` can be used to get the number of rows in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 242,
      "metadata": {
        "id": "VwUCeGNR4-MN"
      },
      "outputs": [],
      "source": [
        "# Submit to Coursemology\n",
        "def split_dataset(dataset):\n",
        "    '''\n",
        "    dataset: numpy array, shape = [N, D]\n",
        "    RETURN\n",
        "        train_X: numpy array, shape = [int(N * 0.8), D-1]\n",
        "        test_X: numpy array, shape = [N-int(N * 0.8), D-1]\n",
        "        train_y: numpy array, shape = [int(N * 0.8), 1]\n",
        "        test_y: numpy array, shape = [N-int(N * 0.8), 1]\n",
        "    '''\n",
        "    r, c = dataset.shape\n",
        "    train_r = int(0.8*r)\n",
        "    train_X, test_X = dataset[:train_r, :c-1], dataset[train_r:, :c-1]\n",
        "    train_y, test_y = dataset[:train_r, -1].reshape(-1, 1), dataset[train_r:, -1].reshape(-1, 1)\n",
        "    ## start your code here\n",
        "    \n",
        "    \n",
        "    ## end \n",
        "    return train_X, test_X, train_y, test_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 243,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hABMkOM9p0Ie",
        "outputId": "2d6cac89-e1e6-4055-c070-ffab1ca51b40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(404, 14) (102, 14) (404, 1) (102, 1)\n"
          ]
        }
      ],
      "source": [
        "# Testing\n",
        "\n",
        "train_X, test_X, train_y, test_y = split_dataset(dataset_ones)\n",
        "print(train_X.shape, test_X.shape, train_y.shape, test_y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWp5OQqCT-Ek"
      },
      "source": [
        "Expected output:\n",
        "```\n",
        "(404, 14) (102, 14) (404, 1) (102, 1)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABoEeHOR56_5"
      },
      "source": [
        "---\n",
        "\n",
        "### Q5 model (2 marks)\n",
        "\n",
        "We can start to build our linear regression model.\n",
        "\n",
        "For a dataset with N rows and D columns, the function `model` takes in n array of x values `X` (of shape N x D) and an array of weights `W` (of shape D x 1) and returns an array of predicted values for each data sample `prediction` (of shape N x 1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 244,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Submit to Coursemology\n",
        "def model(X, W):\n",
        "    '''\n",
        "    X: numpy array, shape = [N, D]\n",
        "    W: numpy array, shape = [D, 1]\n",
        "    RETURN\n",
        "        prediction: numpy array, shape = [N, 1]\n",
        "    '''\n",
        "    prediction = np.matmul(W,X)\n",
        "    ## start your code here\n",
        "    \n",
        "    \n",
        "    ## end\n",
        "    return prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 245,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7gtmwk074Dm",
        "outputId": "6fc76b45-7d16-44d0-85a9-85de0b6759a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(506, 1)\n"
          ]
        }
      ],
      "source": [
        "# Testing\n",
        "\n",
        "y_hat = model(np.zeros((dataset.shape[1],1)), dataset_ones[:,:-1])\n",
        "print(y_hat.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W15IzCsDnyrO"
      },
      "source": [
        "Expected output:\n",
        "```\n",
        "(506, 1)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOCWb-ksNZ8_"
      },
      "source": [
        "---\n",
        "\n",
        "### Q6 mse_function (3 marks)\n",
        "\n",
        "We need a loss function as a metric for our gradient descent to determine how well our model is doing.\n",
        "\n",
        "The function `mse_function` takes in an array of actual y values `y` and an array of predicted y values `y_hat` and returns an array of mean squared error (MSE) `loss`. This function **should not use loops** as it can be done directly through array operations.\n",
        "\n",
        "Tip: `np.dot()` can be used for matrix multiplication."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 265,
      "metadata": {
        "id": "XhOLIxGHH6KT"
      },
      "outputs": [],
      "source": [
        "# Submit to Coursemology\n",
        "def mse_function(y_hat, y):\n",
        "    '''\n",
        "    y_hat: numpy array, shape = [N, 1]\n",
        "    y: numpy array, shape = [N, 1]\n",
        "    RETURN\n",
        "        loss: float value\n",
        "    '''\n",
        "    loss = np.mean(np.power(y_hat-y, 2))\n",
        "    ## start your code here\n",
        "    \n",
        "    \n",
        "    ## end \n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "askXs6-iN0NY"
      },
      "source": [
        "When you run this code, you should get the expected output as shown below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 266,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_mAun-05M6r",
        "outputId": "877a6653-bf5a-4b55-c6da-1ee84e73ca6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "592.1469169960474\n"
          ]
        }
      ],
      "source": [
        "# Testing\n",
        "\n",
        "N = dataset_ones.shape[1]\n",
        "print(mse_function(y_hat,dataset_ones[:,-1:]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nG9QH9tBVoOb"
      },
      "source": [
        "Expected output:\n",
        "```\n",
        "592.1469169960474\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJUGpmlP-W4b"
      },
      "source": [
        "---\n",
        "\n",
        "### Q7 gradient (4 marks)\n",
        "\n",
        "The next step is to produce our gradients so that we can update our weights. This can be done in the following steps:\n",
        "1. Compute y_hat (of shape R x 1)\n",
        "2. Compute difference between y_hat and actual y\n",
        "3. Multiply X by the difference in step 2\n",
        "4. Divide by number of rows N to get gradient\n",
        "\n",
        "The function `gradient` takes in x values `X` (of shape N x D), weights `W` (of shape D x 1),  and y values `y` (of shape N x 1) and returns an array of gradients for each weight `grad`. Return values to the **nearest 6 decimal places using `round()`**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 296,
      "metadata": {
        "id": "WzJLJ8tX-ZNI"
      },
      "outputs": [],
      "source": [
        "# Submit to Coursemology\n",
        "def gradient(X, W, y):\n",
        "    '''\n",
        "    X: numpy array, shape = [N, D]\n",
        "    W: numpy array, shape = [D, 1]\n",
        "    y: numpy array, shape = [N, 1]\n",
        "    RETURN\n",
        "        grad: numpy array, shape = [D, 1]\n",
        "    '''\n",
        "    y_hat = np.dot(X,W)\n",
        "    loss = y-y_hat\n",
        "    temp = X * loss\n",
        "    grad = np.sum((1/X.shape[0]) * temp, axis=0).reshape(W.shape[0], 1)\n",
        "    return grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 297,
      "metadata": {
        "id": "31PeCl4U-lxR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[-45.5], [-64.0], [-82.5]]\n"
          ]
        }
      ],
      "source": [
        "# Testing\n",
        "\n",
        "resultlist = gradient(np.array([[1,2,3],[3,4,5]]),np.array([[5],[2],[1]]),np.array([[2],[1]])).tolist()\n",
        "print(resultlist)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTpmMuRuVmQ9"
      },
      "source": [
        "\n",
        "Expected output:\n",
        "```\n",
        "[[-45.5], [-64.0], [-82.5]]\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eR0D8v-2G0e7"
      },
      "source": [
        "---\n",
        "\n",
        "### Q8 gradient_descent (3 marks)\n",
        "\n",
        "With everything else settled, we can create our gradient descent function.\n",
        "\n",
        "The function `gradient_descent` takes in the training data `train_X` and `train_y`, the testing data `test_X` and `test_y`, the learning rate `L` and the number iterations to run `num_iter`. It returns the array of optimal parameters `best_W` and the a dictionary tracking the training data loss and testing data loss `loss_history`.\n",
        "\n",
        "You only need to implement the updating of weights in each iteration as given in the following steps:\n",
        "1. Predict y values using the weights `W` and x values `train_X` (remember to use only the **training data**)\n",
        "2. Calculate the gradient of the loss function\n",
        "3. Update the weights `W`\n",
        "\n",
        "A correct implementation of `gradient` has been given to you in Coursemology (i.e. you don't need to code it again)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 300,
      "metadata": {
        "id": "ptsENRpDY1Yg"
      },
      "outputs": [],
      "source": [
        "# Submit to Coursemology\n",
        "def gradient_descent(train_X, train_y, test_X, test_y, L, num_iter):  \n",
        "    '''\n",
        "    train_X: numpy array, shape = [N_train, D]\n",
        "    test_X: numpy array, shape = [N_test, D]\n",
        "    train_y: numpy array, shape = [N_train, 1]\n",
        "    test_y: numpy array, shape = [N_test, 1]\n",
        "    L: float value\n",
        "    num_iter: int value\n",
        "    RETURN\n",
        "        best_W: numpy array, shape = [D, 1]\n",
        "        loss_history: numpy array, shape = [2, num_iter]\n",
        "    '''\n",
        "    N_train, D = train_X.shape # number of training samples, number of features\n",
        "    W = np.zeros((D,1)) # the parameters of the linear regression model\n",
        "    best_W = None # the optimized parameters on the training data \n",
        "\n",
        "    # record loss of each iteration - both rmse between training data and target\n",
        "    # data as well as between testing data and target data.\n",
        "    loss_history = { 'train': [], 'test': [] }\n",
        "\n",
        "    # Gradient descent steps\n",
        "    for i in range(num_iter): \n",
        "        ## start your code here\n",
        "        W = W + L * gradient(train_X, W,train_y)\n",
        "        ## end of your code\n",
        "\n",
        "        train_y_hat = model(W, train_X)\n",
        "        test_y_hat = model(W, test_X)\n",
        "        \n",
        "        train_loss = mse_function(train_y_hat, train_y)\n",
        "        test_loss = mse_function(test_y_hat, test_y)\n",
        "\n",
        "        if len(loss_history['test']) > 0 and test_loss < loss_history['test'][-1]: # better test loss\n",
        "            best_W = np.copy(W) # store the optimal parameter\n",
        "        \n",
        "        loss_history['train'].append(train_loss)\n",
        "        loss_history['test'].append(test_loss)\n",
        "\n",
        "    return best_W, loss_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 301,
      "metadata": {
        "id": "qiQng79P5TxH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 9.93008179 -1.09184095  4.62616462 -1.4178688   3.02227804 -0.43412238\n",
            "  11.71497134  1.5884702   2.62489101 -0.92561114 -1.72612043 -1.62906862\n",
            "   9.52457346 -5.8227015 ]]\n",
            "17.167879004711903\n"
          ]
        }
      ],
      "source": [
        "# Testing\n",
        "\n",
        "best_W, loss_history = gradient_descent(train_X, train_y, test_X, test_y, 0.01, 5000)\n",
        "print(best_W.T)\n",
        "print(mse_function(model(best_W, test_X), test_y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88UhqfFGISOv"
      },
      "source": [
        "\n",
        "Expected output:\n",
        "\n",
        "```\n",
        "[[ 9.93008179 -1.09184095  4.62616462 -1.4178688   3.02227804 -0.43412238\n",
        "  11.71497134  1.5884702   2.62489101 -0.92561114 -1.72612043 -1.62906862\n",
        "   9.52457346 -5.8227015 ]]\n",
        "17.167879004711896\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDGdGQ9k1Z7t"
      },
      "source": [
        "**Bonus: plotting the learning curve**\n",
        "\n",
        "When you run the following code, you should get something similar to the graph shown below:\n",
        "\n",
        "![graph image here (LRGDgrph1.png)](https://drive.google.com/uc?export=view&id=1-MJzt4mKpwUYHYGfSkmdcqnQdZF6rs7a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 302,
      "metadata": {
        "id": "AZDS87u35W1-"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plot the learning curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x_values = range(1,len(loss_history['train'])+1)\n",
        "plt.plot(x_values, loss_history['train'], 'b', label=\"Traning Loss\")\n",
        "plt.plot(x_values, loss_history['test'], 'r', label=\"Testing Loss\")\n",
        "plt.title(\"Training and Testing Loss\")\n",
        "plt.xlabel(\"number of iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJ1kQ--oARm6"
      },
      "source": [
        "---\n",
        "\n",
        "### Q9 Prediction using the Linear Regression Model (2 marks)\n",
        "\n",
        "Let's test our linear regression model.\n",
        "\n",
        "Given new data `new_X` and the optimal parameters `best_W` (from Q8), use the `model` function from Q5 to predict the price. A correct implementation of `model` has been given to you in Coursemology (i.e. you don't need to code it again)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 303,
      "metadata": {
        "id": "aruXXS2KAUKM"
      },
      "outputs": [],
      "source": [
        "# the test data below is already normalzied\n",
        "new_dataset = np.asarray([[1.000,0.001,0.125,0.272,0.000,0.286,0.470,0.656,0.403,0.174,0.237,0.277,0.997,0.295,22.900],\n",
        "[1.000,0.002,0.000,0.236,0.000,0.130,0.426,0.313,0.361,0.087,0.088,0.564,1.000,0.343,20.000],\n",
        "[1.000,0.001,0.125,0.206,0.000,0.049,0.390,0.349,0.488,0.130,0.302,0.670,1.000,0.313,17.400],\n",
        "[1.000,0.001,0.000,0.148,0.000,0.132,0.588,0.548,0.301,0.087,0.115,0.628,0.988,0.132,26.600],\n",
        "[1.000,0.001,0.000,0.297,0.000,0.278,0.492,0.848,0.090,0.174,0.376,0.883,0.977,0.341,20.400],\n",
        "[1.000,0.004,0.000,0.786,0.000,0.492,0.555,0.989,0.090,0.130,0.477,0.915,0.995,0.300,19.200],\n",
        "[1.000,0.021,0.000,0.701,1.000,0.453,0.813,0.981,0.083,0.174,0.412,0.223,0.982,0.005,35.000],\n",
        "[1.000,0.001,0.000,0.073,0.000,0.212,0.391,0.895,0.169,0.087,0.011,0.553,0.985,0.338,26.400],\n",
        "[1.000,0.003,0.000,0.371,0.000,0.214,0.530,0.511,0.293,0.130,0.172,0.638,0.995,0.255,24.400],\n",
        "[1.000,0.005,0.000,0.210,0.000,0.245,0.573,0.191,0.204,0.304,0.229,0.511,0.958,0.056,31.500],\n",
        "[1.000,0.004,0.000,0.346,0.000,0.327,0.541,0.662,0.219,0.130,0.223,0.617,0.996,0.238,23.100],\n",
        "[1.000,0.001,0.000,0.102,0.000,0.154,0.442,0.236,0.371,0.130,0.464,0.457,0.964,0.227,19.300],\n",
        "[1.000,0.000,0.850,0.135,0.000,0.091,0.566,0.255,0.673,0.130,0.313,0.564,0.989,0.128,23.100],\n",
        "[1.000,0.043,0.000,0.647,0.000,0.792,0.515,0.908,0.106,1.000,0.914,0.809,0.883,0.344,19.900],\n",
        "[1.000,0.125,0.000,0.647,0.000,0.582,0.258,1.000,0.004,1.000,0.914,0.809,1.000,0.912,13.800],\n",
        "[1.000,0.103,0.000,0.647,0.000,0.648,0.378,1.000,0.041,1.000,0.914,0.809,1.000,0.603,11.300],\n",
        "[1.000,0.097,0.000,0.647,0.000,0.634,0.504,0.924,0.060,1.000,0.914,0.809,1.000,0.371,13.800],\n",
        "[1.000,0.108,0.000,0.647,0.000,0.634,0.545,1.000,0.046,1.000,0.914,0.809,0.948,0.513,12.100],\n",
        "[1.000,0.211,0.000,0.647,0.000,0.436,0.204,1.000,0.039,1.000,0.914,0.809,0.072,0.901,20.900],\n",
        "[1.000,0.083,0.000,0.647,0.000,0.605,0.504,0.774,0.073,1.000,0.914,0.809,0.243,0.546,11.000]]\n",
        ")\n",
        "\n",
        "new_X = new_dataset[:,:-1]\n",
        "new_y = new_dataset[:,-1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 304,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[24.36024917]\n",
            " [22.34836515]\n",
            " [22.56704338]\n",
            " [25.55041309]\n",
            " [21.80564756]\n",
            " [22.20258111]\n",
            " [31.127792  ]\n",
            " [22.59253409]\n",
            " [23.63320466]\n",
            " [24.36141296]\n",
            " [23.84767426]\n",
            " [22.43844523]\n",
            " [29.52878713]\n",
            " [18.96171354]\n",
            " [13.63808166]\n",
            " [16.93558238]\n",
            " [19.70431347]\n",
            " [18.93449087]\n",
            " [ 4.29207349]\n",
            " [11.29896697]]\n"
          ]
        }
      ],
      "source": [
        "# Submit to Coursemology\n",
        "predicted_y = model(best_W, new_X)\n",
        "## start your code here\n",
        "\n",
        "## end\n",
        "\n",
        "print(predicted_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HmgNJsleMLN"
      },
      "source": [
        "Expected output:\n",
        "\n",
        "```\n",
        "[[24.36024917]\n",
        " [22.34836515]\n",
        " [22.56704338]\n",
        " [25.55041309]\n",
        " [21.80564756]\n",
        " [22.20258111]\n",
        " [31.127792  ]\n",
        " [22.59253409]\n",
        " [23.63320466]\n",
        " [24.36141296]\n",
        " [23.84767426]\n",
        " [22.43844523]\n",
        " [29.52878713]\n",
        " [18.96171354]\n",
        " [13.63808166]\n",
        " [16.93558238]\n",
        " [19.70431347]\n",
        " [18.93449087]\n",
        " [ 4.29207349]\n",
        " [11.29896697]]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jCKxWpIAgRq"
      },
      "source": [
        "---\n",
        "\n",
        "### Q10 Calculating MSE (1 mark)\n",
        "\n",
        "Given our predicted y values `predicted_y` and the actual y values `new_y`, use the `mse_function` from Q6 to calculate the MSE of our predictions. A correct implementation of `mse_function` has been given to you in Coursemology (i.e. you don't need to code it again)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 305,
      "metadata": {
        "id": "KTehnZnjAh6F"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "28.468249030219898\n"
          ]
        }
      ],
      "source": [
        "# Submit to Coursemology\n",
        "mse_loss = mse_function(predicted_y, new_y)\n",
        "## Start your code here\n",
        "\n",
        "## end\n",
        "\n",
        "print(mse_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWLM5AIrePOV"
      },
      "source": [
        "Expected output:\n",
        "\n",
        "```\n",
        "28.468249030219887\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-83kPBxBA-n"
      },
      "source": [
        "**Bonus: plotting the actual y values vs the predicted y values**\n",
        "\n",
        "When you run the following code, you should get something similar to the graph shown below:\n",
        "\n",
        "![graph image here (LRGDgrph2.png)](https://drive.google.com/uc?export=view&id=16jCCxZe2nNhsDllafNwpbMd93cy-MSgv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzyfcWj85kSJ"
      },
      "outputs": [],
      "source": [
        "#plot the graph showing the predictedy  values and the actual y values\n",
        "x_values = range(1,predicted_y.shape[0]+1)\n",
        "plt.plot(x_values, new_y, 'b', label=\"actual - y\")\n",
        "plt.plot(x_values, predicted_y, 'r', label=\"predicted - y\")\n",
        "plt.title(\"Actual-y and predicted-y values\")\n",
        "plt.xlabel(\"Test data sample\")\n",
        "plt.ylabel(\"Housing Price in 1000's\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lcW8p2hq4jf"
      },
      "source": [
        "---\n",
        "\n",
        "## Q11 Reflection (3 marks)\n",
        "\n",
        "In this question, list out the difficulties and how you overcome them when doing the assignment. You can also talk about what you have learned and how do you think we should improve the assignment here. Please list your comments in bullet points. **This section is graded**. Note that you won't get a mark when you submit this question, but you will automatically be awarded the full mark when finalising submission (subject to manual marking afterwards)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iN_AWVpzq92u"
      },
      "source": [
        "Please enter your comments here by double-clicking on this text cell:\n",
        "* comment 1\n",
        "* comment 2\n",
        "* etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0OhorHf2laz"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "# End of Assignment"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
